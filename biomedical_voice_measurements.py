# -*- coding: utf-8 -*-
"""ParkinsonsDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_JYmEWhLt79qQLWF2B0rYN2iZ_ETrd9T
"""

#IMPORTS
import pandas as pd #python data analysis library
import numpy as np #best for working with arrays!
import matplotlib.pyplot as plt #Plotting/viz library
import seaborn as sns #data viz library based on matplotlib
#sklearn: popular ML library!
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm #support vector machines
from sklearn.metrics import accuracy_score

##Giving google colab access to your drive!
from google.colab import drive
drive.mount('/content/drive')

##Loading the dataset and dataset description into dataframes
pathData = "/content/drive/MyDrive/Parkinsons2022Fall/parkinsons.csv"
parkinsonsData = pd.read_csv(pathData)

##Row represents unique sound recordings of Patients, about 5/6 sound recordings for the ~50 patients.
##Columns represent sound metrics!!
parkinsonsData ##Status is set to 1!!!

parkinsonsData.shape

parkinsonsData.info()

parkinsonsData.isnull() ##Our dataset is already quite clean!!!

parkinsonsData.isnull().sum()

parkinsonsData.describe()

parkinsonsData['status'].value_counts()
#1 for Parkinsons, 0 for healthy

parkinsonsData.groupby('status').mean() ##grouping data based on the target variable

X = parkinsonsData.drop(columns=['name', 'status'], axis = 1) ##why do we drop these cols?
#hint: does a similar name indicate similar likelihood of parkinsons?
Y = parkinsonsData['status']

print(X)

print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)#splits arrays into random train/test sets



print(X.shape, X_train.shape, X_test.shape)

scaler = StandardScaler()
#z = (x - u) / stddev

scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

model = svm.SVC(kernel='linear')

model.fit(X_train, Y_train)
X_train

X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)

print('Accuracy score of training data: ', training_data_accuracy)

X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)

print('Accuracy score of test data: ', test_data_accuracy) #no signs of overfitting or underfitting

input_data = (197.07600,206.89600,192.05500,0.00289,0.00001,0.00166,0.00168,0.00498,0.01098,0.09700,0.00563,0.00680,0.00802,0.01689,0.00339,26.77500,0.422229,0.741367,-7.348300,0.177551,1.743867,0.085569)


# changing input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)


# standardize the data
std_data = scaler.transform(input_data_reshaped)

prediction = model.predict(std_data)
print(prediction)


if (prediction[0] == 0):
  print("The Person does not have Parkinsons Disease")

else:
  print("The Person has Parkinsons")